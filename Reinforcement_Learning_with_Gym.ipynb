{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "from enum import Enum\n",
    "\n",
    "# Python imports\n",
    "import random\n",
    "import heapq\n",
    "import collections\n",
    "\n",
    "# Reinforcement Learning environments\n",
    "import gymnasium as gym\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "\n",
    "# Plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the default figure size\n",
    "plt.rcParams[\"figure.figsize\"] = [16, 4]\n",
    "\n",
    "from helper import visualize_env, visualize_v, visualize_p, DPType, create_environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cxgvN1YlSnxK"
   },
   "source": [
    "# Reinforcement Learning with Gym\n",
    "\n",
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Reinforcement_learning_diagram.svg/300px-Reinforcement_learning_diagram.svg.png).\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Reinforcement Learning is a special form of machine learning, where an agent interacts with an environment, conducts observations on the effects of actions and collects rewards.\n",
    "\n",
    "The goal of reinforcement learning is to learn an optimal policy, so that given a state an agent is able to decide what it should do next.\n",
    "\n",
    "In today's workshop we will look into three fundamental algorithms that are capable of solving MDPs, namely [Policy Iteration](https://en.wikipedia.org/wiki/Markov_decision_process#Policy_iteration), [Value Iteration](https://en.wikipedia.org/wiki/Markov_decision_process#Value_iteration), and [Q-Learning](https://en.wikipedia.org/wiki/Q-learning).\n",
    "\n",
    "## Objectives\n",
    "\n",
    "After this workshop you should know:\n",
    "\n",
    "- The relevant pieces for a reinforcement learning system\n",
    "- The basics of *[gym](https://gym.openai.com/envs/#classic_control)* to conduct your own RL experiments\n",
    "- The differences of value and policy iteration compared with Q-Learning\n",
    "- How Q-Learning converges towards a stable policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "The following section is based on the de facto standard [Reinforcement Learning: An Introduction\n",
    "](http://incompleteideas.net/book/the-book-2nd.html) by Sutton and Barto.\n",
    "\n",
    "### Markov Decision Process (MDP)\n",
    "An MDP is defined by a tuple $\\left\\langle\\mathcal{S}, \\mathcal{A}, \\mathcal{P}_{s s^{\\prime}}^a, \\mathcal{R}_s^a\\right\\rangle$, with:\n",
    "- $\\mathcal{S}$ being the finite state space,\n",
    "- $\\mathcal{A}$ being the finite action space,\n",
    "- $\\mathcal{P}_{s s^{\\prime}}^a=\\mathbb{P}\\left[s^{\\prime} \\mid a, s\\right]$ being the model of the probability to transition to $s^{\\prime}$ when taking action $a$ in state $s$,\n",
    "\n",
    "- $\\mathcal{R}_s^a=\\mathbb{E}[r \\mid a, s]$ being the model of the expected reward received when taking action $a$ in state $s$.\n",
    "\n",
    "![MPD](mdp.png \"MDP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trajectory\n",
    "The trajectory is defined by a sequence of states and actions as a path through an MDP as\n",
    "\n",
    "$\\tau=\\left(s_0, a_0, s_1, a_1, \\ldots, s_T\\right).$\n",
    "\n",
    "### Return\n",
    "The return $G$ of a trajectory $\\tau$ equals its accumulated discounted reward at time step $t$, taking action $a_t$ in state $s_t$, defined as\n",
    "\n",
    "$\n",
    "G(\\tau)=\\sum_{\\left(s_t, a_t\\right) \\in \\tau} \\gamma^t \\mathcal{R}_{s_t}^{a_t}.\n",
    "$\n",
    "\n",
    "$\\gamma$ is the discount factor, which is a value between 0 and 1, that determines the importance of future rewards.\n",
    "\n",
    "### Policy\n",
    "The policy $\\pi$ is a probability distribution over actions $a \\in \\mathcal{A}(s)$, defined as\n",
    "\n",
    "$\n",
    "\\pi=\\pi(a \\mid s)\n",
    "$\n",
    "\n",
    "The optimal policy $\\pi^*(a \\mid s)$ is a policy that yields the highest state value and state-action value of all policies.\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "V^*(s) & \\geq V^\\pi(s) & & \\forall s \\in \\mathcal{S} \\\\\n",
    "Q^*(s, a) & \\geq Q^\\pi(s, a) & & \\forall s \\in \\mathcal{S}, \\forall a \\in \\mathcal{A}(s)\n",
    "\\end{aligned}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### State Value\n",
    "The recursive form of the state value $V^\\pi(s)$ with an arbitrary policy $\\pi$, is defined as\n",
    "\n",
    "$\n",
    "V^\\pi(s)=\\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s)\\left(\\mathcal{R}_s^a+\\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} \\mathcal{P}_{s s^{\\prime}}^a V^\\pi\\left(s^{\\prime}\\right)\\right) .\n",
    "$\n",
    "\n",
    "The recursive form of the state value $V^*(s)$ with the optimal policy, is defined as\n",
    "\n",
    "$\n",
    "V^*(s)=\\max _{a \\in \\mathcal{A}}\\left(\\mathcal{R}_s^a+\\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} \\mathcal{P}_{s s^{\\prime}}^a V^*\\left(s^{\\prime}\\right)\\right) .\n",
    "$\n",
    "\n",
    "### State-Action Value\n",
    "The recursive form of the state-action value $Q^\\pi(s, a)$ with an arbitrary policy $\\pi$, is defined as\n",
    "\n",
    "$\n",
    "Q^\\pi(s, a)=\\mathcal{R}_s^a+\\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} \\mathcal{P}_{s s^{\\prime}}^a \\sum_{a^{\\prime} \\in \\mathcal{A}} \\pi\\left(a^{\\prime} \\mid s^{\\prime}\\right) Q^\\pi\\left(s^{\\prime}, a^{\\prime}\\right) .\n",
    "$\n",
    "\n",
    "The recursive form of the state-action value $Q^*(s, a)$ with the optimal policy, is defined as\n",
    "\n",
    "$\n",
    "Q^*(s, a)=\\mathcal{R}_s^a+\\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} \\mathcal{P}_{s s^{\\prime}}^a \\max _{a^{\\prime} \\in \\mathcal{A}} Q^*\\left(s^{\\prime}, a^{\\prime}\\right) .\n",
    "$\n",
    "\n",
    "The equations provided describe the general cases of the state-value and state-action-value functions for both arbitrary and optimal policies in a stochastic environment. In a deterministic environment, some simplifications can be made, as the probabilities involved are either 0 or 1.\n",
    "\n",
    "This simplifies the equations for the state-value and state-action-value functions.\n",
    "\n",
    "#### Deterministic State Value\n",
    "\n",
    "For a deterministic environment, the transition probabilities $\\mathcal{P}_{s s^{\\prime}}^a$ are either 0 or 1, reflecting that there's a certain next state $s^{\\prime}$ for every state-action pair $(s, a)$. Also, the policy $\\pi(a | s)$ becomes deterministic, picking a single action with probability 1.\n",
    "\n",
    "The recursive form of the state value function under an arbitrary deterministic policy $pi$ becomes:\n",
    "\n",
    "$ V^\\pi(s) = \\mathcal{R}_s^{\\pi(s)} + \\gamma V^\\pi\\left(f(s, \\pi(s))\\right) $\n",
    "\n",
    "where $\\pi(s)$ is the action chosen by policy $\\pi$ in state $s$, and $f(s, \\pi(s))$ is the deterministic transition function giving the next state.\n",
    "\n",
    "The recursive form of the state value function under the optimal policy becomes:\n",
    "\n",
    "$ V^*(s) = \\max_{a \\in \\mathcal{A}}\\left(\\mathcal{R}_s^a + \\gamma V^*\\left(f(s, a)\\right)\\right) $\n",
    "\n",
    "#### Deterministic State-Action Value\n",
    "\n",
    "Similar simplifications apply. The recursive form of the state-action value function under an arbitrary deterministic policy $\\pi$ becomes:\n",
    "\n",
    "$ Q^\\pi(s, a) = \\mathcal{R}_s^a + \\gamma Q^\\pi\\left(f(s, a), \\pi(f(s, a))\\right) $\n",
    "\n",
    "And under the optimal policy:\n",
    "\n",
    "$ Q^*(s, a) = \\mathcal{R}_s^a + \\gamma \\max_{a^{\\prime} \\in \\mathcal{A}} Q^*\\left(f(s, a), a^{\\prime}\\right) $\n",
    "\n",
    "These simplifications essentially reflect the deterministic nature of the environment and the policy, removing the summations over states and actions that are present in the stochastic case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NvdmBl8GajjF"
   },
   "source": [
    "### Environment | [Frozen Lake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/#frozen-lake)\n",
    "\n",
    "Winter has arrived. You and your friends are by the lake, its surface mostly ice-covered. However, a few spots where the ice has thinned reveal the cold water beneath. Amidst the frosty expanse, an exciting object catches your eye on the lake, and you feel an urge to pick it up.\n",
    "\n",
    "![Frozen Lake](frozen_lake_dalle3.png \"Frozen Lake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BzfpVLxA-T4W"
   },
   "source": [
    "Create and visualize the desired environment with the selected name. You can choose between the following environments: `FrozenLake-v1` and `FrozenLake8x8-v1`, the latter being a larger version of the former."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gsl3GswnX1I6"
   },
   "outputs": [],
   "source": [
    "env = create_environment(\"FrozenLake-v1\")\n",
    "\n",
    "env.reset()\n",
    "_, ax = plt.subplots()\n",
    "visualize_env(env, ax=ax, title=\"Frozen Lake Environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the Environment (Object)\n",
    "\n",
    "**TODO :**\n",
    "Analyze the environment object and figure out its *observation/state-* and *actionspace* as well as its *reward range*.\n",
    "\n",
    "What is the size of the observation/state space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the size of the action space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the range of rewards?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Action Space\n",
    "\n",
    "When designing Reinforcement Learning agents, defining a clear and manageable action space is essential. Utilizing Python's `Enum` class for this purpose is beneficial for several reasons:\n",
    "\n",
    "1. **Readability**: Actions can be referred to by descriptive names instead of numeric values.\n",
    "2. **Maintainability**: If the action space changes, using named actions can help prevent errors from magic numbers.\n",
    "3. **Flexibility**: Enumerations can be iterated over, allowing for easy examination and manipulation of the action space.\n",
    "\n",
    "Below, we define an `Action` enumeration representing the possible moves in the FrozenLake environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action(Enum):\n",
    "    \"\"\"\n",
    "    An enumeration for the possible actions in the FrozenLake environment.\n",
    "    \"\"\"\n",
    "\n",
    "    LEFT = 0\n",
    "    DOWN = 1\n",
    "    RIGHT = 2\n",
    "    UP = 3\n",
    "\n",
    "\n",
    "for action in Action:\n",
    "    print(\"{:10} = {}\".format(action.name, action.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Reinforcement Learning, interacting with the environment is a core concept. When an agent takes an action, the environment responds by transitioning to a new state and providing a reward. It's essential to understand this dynamic to design effective agents.\n",
    "\n",
    "\n",
    "Let's analyze the action space of the environment and what happens when we take an action.\n",
    "\n",
    "*Initial State:* We'll reset the environment to its initial state and display it.\n",
    "\n",
    "*Action Exploration:*\n",
    "- For each action in our action space: \n",
    "    - We'll reset the environment to its starting state to ensure consistency.\n",
    "    - Take the action and observe the new state, reward, and whether the episode is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "colab_type": "code",
    "id": "F5OmhQ8sVLHK",
    "outputId": "167f15d0-f60c-43af-ebee-14e265d552bd"
   },
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(2, 3, figsize=(12, 6))\n",
    "\n",
    "s, _ = env.reset()\n",
    "print(f\"the initial state is: {s}\")\n",
    "visualize_env(env, ax=axs[0, 0], title=f\"Start | state is: {s}\")\n",
    "\n",
    "axs = axs.reshape(-1)[1:]\n",
    "axs[-1].axis(\"off\")\n",
    "\n",
    "for action, ax in zip(Action, axs):\n",
    "    env.reset()\n",
    "    # skip the first axis\n",
    "    print(f\"executing action {action.value}, should go {action.name}\")\n",
    "    s1, r, d, _, _ = env.step(action.value)\n",
    "    print(f\"new state is: {s1} done: {d}\")\n",
    "    visualize_env(env, ax=ax, title=f\"{action.name} | state is: {s1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy and Value Iteraton Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UxwwTshweK8i"
   },
   "outputs": [],
   "source": [
    "MAX_ITERATIONS = (\n",
    "    1000  # Max number of iterations to run the algorithms for (to avoid infinite loops)\n",
    ")\n",
    "DISCOUNT_FACTOR = 0.95  # Discount factor to compute the return for a trajectory (gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Return\n",
    "The trajectory generated by a policy $\\pi$ is defined as $\\tau^\\pi=\\left(s_0, a_0, s_1, a_1, \\ldots, s_T\\right)$. Based on it we can evaluate the policy by computing the total return $G(\\tau^\\pi)$, which allows us to compare different policies.\n",
    "\n",
    "$\n",
    "G(\\tau^\\pi)=\\sum_{\\left(s_t, a_t\\right) \\in \\tau} \\gamma^t \\mathcal{R}_{s_t}^{a_t}.\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FH2aMLY_jrjQ"
   },
   "outputs": [],
   "source": [
    "def calculate_policy_return(env, policy: np.ndarray, discount_factor: float) -> float:\n",
    "    \"\"\"Evaluates a policy by running it until termination and collect its reward\"\"\"\n",
    "    state, _ = env.reset()\n",
    "    total_return = 0.0\n",
    "    step = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        state, reward, done, *_ = env.step(policy[state])\n",
    "        #### TODO: Calculate the total return of the episode\n",
    "        total_return += discount_factor**step * reward\n",
    "        step += 1\n",
    "    return total_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONVERGENCE_THRESHOLD = 1e-4\n",
    "\n",
    "\n",
    "def policy_evaluation(\n",
    "    env, policy: np.ndarray, discount_factor: float, dp_type: DPType\n",
    ") -> Tuple[np.ndarray, int]:\n",
    "    \"\"\"Iteratively evaluate the value function under the given policy\"\"\"\n",
    "    # Initialize the state value function with zeros\n",
    "    v = np.zeros(env.observation_space.n)\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        # Copy the state value function to check for convergence and avoid mixing old and new values in the updates\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.observation_space.n):\n",
    "            if dp_type == DPType.POLICY_ITERATION:\n",
    "                v[s] = evaluate_action(env, s, prev_v, policy, discount_factor)\n",
    "            elif dp_type == DPType.VALUE_ITERATION:\n",
    "                v[s] = evaluate_max_action(env, s, prev_v, discount_factor)\n",
    "        if value_functions_converged(v, prev_v):\n",
    "            break\n",
    "    return v, iteration\n",
    "\n",
    "\n",
    "def evaluate_action(\n",
    "    env,\n",
    "    s: int,\n",
    "    prev_v: np.ndarray,\n",
    "    policy: np.ndarray,\n",
    "    discount_factor: float,\n",
    ") -> float:\n",
    "    # Retrieve the action under the current policy\n",
    "    a = policy[s]\n",
    "    (\n",
    "        expected_reward,\n",
    "        expected_discounted_return,\n",
    "    ) = calculate_expected_reward_and_discounted_return(\n",
    "        env, s, a, prev_v, discount_factor\n",
    "    )\n",
    "    # Calculate the v value\n",
    "    return expected_reward + expected_discounted_return\n",
    "\n",
    "\n",
    "def evaluate_max_action(\n",
    "    env, s: int, prev_v: np.ndarray, discount_factor: float\n",
    ") -> float:\n",
    "    # Initialize the action value function\n",
    "    q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    # Iterate over each action\n",
    "    for a in range(env.action_space.n):\n",
    "        (\n",
    "            expected_reward,\n",
    "            expected_discounted_return,\n",
    "        ) = calculate_expected_reward_and_discounted_return(\n",
    "            env, s, a, prev_v, discount_factor\n",
    "        )\n",
    "        # Calculate the Q-Value\n",
    "        q[s, a] = expected_reward + expected_discounted_return\n",
    "    ### TODO: Define the value function and the policy with respect to q\n",
    "    # Choose the max q value over all actions\n",
    "    return np.max(q[s, :])\n",
    "\n",
    "\n",
    "def calculate_expected_reward_and_discounted_return(\n",
    "    env, s: int, a: int, prev_v: np.ndarray, discount_factor: float\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Calculates expected reward and expected discounted return for a state-action pair.\"\"\"\n",
    "    expected_reward, expected_discounted_return = 0.0, 0.0\n",
    "    # Calculate the expected reward and the expected discounted return | p = probability of the transition\n",
    "    for p, s1, r, _ in env.unwrapped.P[s][a]:\n",
    "        ## TODO: Define the expected_reward and the expected_discounted_return\n",
    "        expected_reward += p * r\n",
    "        expected_discounted_return += discount_factor * p * prev_v[s1]\n",
    "    return expected_reward, expected_discounted_return\n",
    "\n",
    "\n",
    "def value_functions_converged(v: np.ndarray, prev_v: np.ndarray) -> bool:\n",
    "    \"\"\"Check if the value functions have converged\"\"\"\n",
    "    return np.sum((np.fabs(prev_v - v))) <= CONVERGENCE_THRESHOLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(\n",
    "    env, v: np.ndarray, policy: np.ndarray, discount_factor: float\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Improve the policy given a value-function\"\"\"\n",
    "    # Initialize the policy\n",
    "    policy = np.zeros(env.observation_space.n, dtype=int)\n",
    "    # Initialize the action value function\n",
    "    q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    for s in range(env.observation_space.n):\n",
    "        for a in range(env.action_space.n):\n",
    "            q[s, a] = np.sum(\n",
    "                [\n",
    "                    p * (r + discount_factor * v[s1])\n",
    "                    for p, s1, r, _ in env.unwrapped.P[s][a]\n",
    "                ]\n",
    "            )\n",
    "        policy[s] = np.argmax(q[s, :])\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "**1. Initialization:**\n",
    "\n",
    "$ V(s) \\in \\mathbb{R}, \\pi(s) \\in A(s) \\text{ arbitrarily for all } s \\in S $\n",
    "\n",
    "**2. Policy Evaluation:**\n",
    "\n",
    "$\\begin{aligned}\n",
    "& \\text{Repeat} \\\\\n",
    "& \\Delta \\leftarrow 0 \\\\\n",
    "& \\text{For each } s \\in S: \\\\\n",
    "& \\quad v \\leftarrow V^\\pi(s) \\\\\n",
    "& \\quad V^\\pi(s) \\leftarrow \\mathcal{R}_s^{\\pi(s)}+\\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} \\mathcal{P}_{s s^{\\prime}}^{\\pi(s)} V^\\pi\\left(s^{\\prime}\\right) \\\\\n",
    "& \\quad \\Delta \\leftarrow \\max(\\Delta, |v - V^\\pi(s)|) \\\\\n",
    "& \\text{until } \\Delta < \\theta \\text{ (a small positive number)}\n",
    "\\end{aligned}$\n",
    "\n",
    "**3. Policy Improvement:**\n",
    "\n",
    "$\\begin{aligned}\n",
    "& \\text{policy-stable} \\leftarrow \\text{true} \\\\\n",
    "& \\text{For each } s \\in S: \\\\\n",
    "& \\quad \\text{old-action} \\leftarrow \\pi(s) \\\\\n",
    "& \\quad \\pi(s) \\leftarrow \\text{argmax}_a \\left(\\mathcal{R}_s^{a}+\\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} \\mathcal{P}_{s s^{\\prime}}^{a} V^\\pi\\left(s^{\\prime}\\right)\\right) \\\\\n",
    "& \\quad \\text{If old-action} \\neq \\pi(s), \\text{ then policy-stable} \\leftarrow \\text{false} \\\\\n",
    "& \\text{If policy-stable, then stop and return } V \\approx v*, \\pi \\approx \\pi*; \\text{ else go to 2}\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "**TODO:**\n",
    "Add the missing steps for the policy iteration algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "colab_type": "code",
    "id": "B1PWKWKVjQbI",
    "outputId": "5f500ce3-d8d6-49d1-ce4c-aa77b0024a80"
   },
   "outputs": [],
   "source": [
    "def policy_iteration(\n",
    "    env, discount_factor: float, max_iterations: int\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Policy-Iteration algorithm\"\"\"\n",
    "    # STEP 1\n",
    "    # Initialize the policy with zeros\n",
    "    policy = np.zeros(env.observation_space.n, dtype=int)\n",
    "    for i in range(max_iterations):\n",
    "        # STEP 2\n",
    "        ### TODO: Evaluate the current policy until convergence of the value function\n",
    "        v, iteration = policy_evaluation(\n",
    "            env, policy, discount_factor, DPType.POLICY_ITERATION\n",
    "        )\n",
    "        # STEP 3\n",
    "        ### TODO: Improve the policy with respect to the current value function by greedily choosing the best action until convergence\n",
    "        new_policy = policy_improvement(env, v, policy, discount_factor)\n",
    "        if np.all(policy == new_policy):\n",
    "            print(f\"Policy-Iteration converged at iteration #{i}\")\n",
    "            break\n",
    "\n",
    "        # Plot the current value function and policy\n",
    "        new_v, _ = policy_evaluation(\n",
    "            env, new_policy, discount_factor, DPType.POLICY_ITERATION\n",
    "        )\n",
    "        _, ax = plt.subplots(1, 2)\n",
    "        visualize_v(env, v, ax[0], f\"#Policy Evaluations {iteration}\")\n",
    "        visualize_p(env, new_v, new_policy, Action, ax[1], f\"Policy Improvement #{i+1}\")\n",
    "        policy = new_policy\n",
    "    return policy, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the algorithm and evaluate the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the optimal value function and policy given the model of the environment\n",
    "policy_opt, v_opt = policy_iteration(env, DISCOUNT_FACTOR, MAX_ITERATIONS)\n",
    "\n",
    "# Evalutate the found value function and policy given the model of the environment\n",
    "policy_return = calculate_policy_return(env, policy_opt, DISCOUNT_FACTOR)\n",
    "print(f\"Average return of the policy: {policy_return:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "**1. Initialization:**\n",
    "\n",
    "$ V(s) \\in \\mathbb{R}, \\pi(s) \\in A(s) \\text{ arbitrarily for all } s \\in S $\n",
    "\n",
    "**2. Policy Evaluation (with maximization -> optimal value function):**\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "& \\text{Repeat} \\\\\n",
    "& \\Delta \\leftarrow 0 \\\\\n",
    "& \\text{For each } s \\in S: \\\\\n",
    "& \\quad v \\leftarrow V^\\pi(s) \\\\\n",
    "& \\quad V^\\pi(s) \\leftarrow \\max_a \\left(\\mathcal{R}_s^{a}+\\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} \\mathcal{P}_{s s^{\\prime}}^{a} V^\\pi\\left(s^{\\prime}\\right)\\right) \\\\\n",
    "& \\quad \\Delta \\leftarrow \\max(\\Delta, |v - V^\\pi(s)|) \\\\\n",
    "& \\text{until } \\Delta < \\theta \\text{ (a small positive number)} \\\\\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "\n",
    "**3. Policy Inference:**\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "& \\text{Output a deterministic policy, } \\pi \\approx \\pi^*, \\text{ such that} \\\\\n",
    "& \\pi(s) = \\text{argmax}_a\\left(\\mathcal{R}_s^{a}+\\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} \\mathcal{P}_{s s^{\\prime}}^{a} V^\\pi\\left(s^{\\prime}\\right)\\right) \\\\\n",
    "\\end{aligned}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lZ-utKPdeSw_"
   },
   "source": [
    "### Algorithm\n",
    "**TODO:**\n",
    "Add the missing calculations for the *expected_reward* the *expected_discounted_return*, *v[s]* and *policy[s]*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(\n",
    "    env, discount_factor: float, max_iterations: int\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Value-Iteration algorithm\"\"\"\n",
    "    # STEP 1\n",
    "    # Initialize the policy with zeros\n",
    "    policy = np.zeros(env.observation_space.n, dtype=int)\n",
    "    for i in range(max_iterations):\n",
    "        # STEP 2\n",
    "        ### TODO: Evaluate the current policy until convergence of the value function\n",
    "        v, iteration = policy_evaluation(\n",
    "            env, policy, discount_factor, DPType.VALUE_ITERATION\n",
    "        )\n",
    "        ### TODO: Define the new policy\n",
    "        new_policy = policy_improvement(env, v, policy, discount_factor)\n",
    "\n",
    "        if np.all(policy == new_policy):\n",
    "            print(f\"Policy-Iteration converged at iteration #{i}\")\n",
    "            break\n",
    "\n",
    "        # Plot the current policy\n",
    "        new_v, _ = policy_evaluation(\n",
    "            env, new_policy, discount_factor, DPType.POLICY_ITERATION\n",
    "        )\n",
    "        _, ax = plt.subplots(1, 2)\n",
    "        visualize_v(env, v, ax[0], f\"#Policy Evaluations {iteration}\")\n",
    "        visualize_p(\n",
    "            env, new_v, new_policy, Action, ax[1], f\"Policy Improvement #{i + 1}\"\n",
    "        )\n",
    "        policy = new_policy\n",
    "    return policy, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the algorithm and evaluate the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the optimal value function and policy given the model of the environment\n",
    "policy_opt, v_opt = value_iteration(env, DISCOUNT_FACTOR, MAX_ITERATIONS)\n",
    "\n",
    "# Evalutate the found value function and policy given the model of the environment\n",
    "policy_return = calculate_policy_return(env, policy_opt, DISCOUNT_FACTOR)\n",
    "print(f\"Average return of the policy: {policy_return:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "& \\text{Initialize } Q(s, a), \\text{ for all } s \\in S, a \\in A(s), \\text{ arbitrarily, and } Q(\\text{terminal-state, }\\cdot) = 0 \\\\\n",
    "& \\text{Repeat (for each episode):} \\\\\n",
    "& \\quad \\text{Initialize } S \\\\\n",
    "& \\quad \\text{Repeat (for each step of episode):} \\\\\n",
    "& \\quad \\quad \\text{Choose } A \\text{ from } S \\text{ using policy derived from } Q \\text{ (e.g., } \\varepsilon\\text{-greedy)} \\\\\n",
    "& \\quad \\quad \\text{Take action } A, \\text{ observe } R, S' \\\\\n",
    "& \\quad \\quad Q(S, A) \\leftarrow Q(S, A) + \\alpha [R + \\gamma \\max_a Q(S', a) - Q(S, A)] \\\\\n",
    "& \\quad \\quad S \\leftarrow S' \\\\\n",
    "& \\quad \\text{until } S \\text{ is terminal}\n",
    "\\end{aligned}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Difference Error\n",
    "### $\\delta_t = \\underbrace{r_{t}}_{\\text{reward}} + \\underbrace{\\gamma}_{\\text{discount factor}} \\cdot \\underbrace{\\max_{a}Q(s_{t+1}, a)}_{\\text{estimate of optimal future value}} - \\underbrace{Q(s_{t}, a_{t})}_{\\text{estimate of optimal current value}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Difference Update\n",
    "### $Q^{new}(s_{t},a_{t}) \\leftarrow \\underbrace{Q(s_{t},a_{t})}_{\\text{old value}} + \\underbrace{\\alpha}_{\\text{learning rate}} \\cdot \\underbrace{\\delta_t}_\\text{temporal difference error}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transiton Dataclass\n",
    "For ease of use we define a transition dataclass that allows us to combine all the relevant information from one state to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Transition:\n",
    "    p: float  # priority (only needed for (prioritized) experience replay)\n",
    "    s: int  # state\n",
    "    a: int  # action\n",
    "    s1: int  # successor state\n",
    "    r: float  # reward\n",
    "    td_e: float  # temporal difference error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Replay Memory and Prioritized Experience Replay (optional)\n",
    "\n",
    "Experience Replay and prioritization of specific experiences are common techniques to make the training more data efficient.\n",
    "\n",
    "* [Paper - Experience Replay, 1992](https://link.springer.com/content/pdf/10.1007%2FBF00992699.pdf)\n",
    "* [Paper - Prioritized Experience Replay, 2015](https://arxiv.org/abs/1511.05952)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, config):\n",
    "        # transitions memory\n",
    "        self.transitions = []\n",
    "        # size of the memory\n",
    "        self.memory_size = config.memory_size\n",
    "        # size of the batches\n",
    "        self.batch_size = config.batch_size\n",
    "        # flag for prioritized experience replay\n",
    "        self.prioritized = config.prioritized\n",
    "\n",
    "    def push(self, transition: Transition):\n",
    "        # if the memory is not yet full add the new transition\n",
    "        if len(self.transitions) < self.memory_size:\n",
    "            heapq.heappush(self.transitions, transition)\n",
    "        # if the memory is full remove the smallest transition and add the new transition\n",
    "        else:\n",
    "            del self.transitions[-1]\n",
    "            heapq.heappush(self.transitions, transition)\n",
    "\n",
    "    def replay(self):\n",
    "        if self.prioritized:\n",
    "            return heapq.nsmallest(self.batch_size, self.transitions)\n",
    "        else:\n",
    "            return random.sample(sorted(self.transitions), self.batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.transitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Q Agent\n",
    "So far we have only defined simple function calls with\n",
    "```python\n",
    "def function_name(arg1, arg2):\n",
    "    # compute something with arg1 and arg2 and return something\n",
    "    if arg2 > 0:\n",
    "        something = other_function(arg1) - arg2\n",
    "    else:\n",
    "        something = arg1\n",
    "    return something\n",
    "```\n",
    "However for more complex tasks it is advisable to write object oriented code using classes. Classes provide a means of bundling data and functionality together. Creating a new class creates a new type of object, allowing new instances of that type to be made. Each class instance can have attributes attached to it for maintaining its state. Class instances can also have methods (defined by its class) for modifying its state.\n",
    "\n",
    "\n",
    "\n",
    "Hence we create a class **QAgent** that incorporates all the methods needed for Q-Learning.\n",
    "\n",
    "```python\n",
    "class QAgent:\n",
    "    \n",
    "    def __init__(self): # constructor method that gets called when the object is being created\n",
    "        \n",
    "    def td_error(self): # Temporal Difference Error\n",
    "        \n",
    "    def td_update(self): # Temporal Difference Update\n",
    "    \n",
    "    def train(self, env): # Train the agent     \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO :**\n",
    "Add the missing formulas for the TD-error and the TD-update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent:\n",
    "    def __init__(self, config):\n",
    "        # Maximum length of training\n",
    "        self.training_length = config.training_length\n",
    "        # Maximum length of an episode\n",
    "        self.episode_length = config.episode_length\n",
    "        # TD error update step size\n",
    "        self.learning_rate = config.learning_rate\n",
    "        # Discount factor for the return calculation\n",
    "        self.discount_factor = config.discount_factor\n",
    "\n",
    "    def td_error(self, q: float, s: int, a: int, s1: int, r: float) -> float:\n",
    "        \"\"\"Calculates the temporal difference error given the current model and transition\"\"\"\n",
    "        ### TODO: return the TD-Error for the given state, action, successor state and reward\n",
    "        td_e = r + self.discount_factor * np.max(q[s1, :]) - q[s, a]\n",
    "        return td_e\n",
    "\n",
    "    def td_update(self, q: float, td_e: float) -> float:\n",
    "        \"\"\"Calculates the adjusted action value (q) given the td error from a single transition\"\"\"\n",
    "        ### TODO: return the update for the q value given the td error and the current q value\n",
    "        q = q + self.learning_rate * td_e\n",
    "        return q\n",
    "\n",
    "    def epsilon_greedy_noise(self, env, s: int, episode: int) -> Tuple[int, float]:\n",
    "        epsilon = np.random.randn(1, env.action_space.n) * (1.0 / (episode + 1))\n",
    "        a = np.argmax(self.q_target[s, :] + epsilon)\n",
    "        return a, epsilon\n",
    "\n",
    "    def epsilon_greedy_linear(self, env, s: int, episode: int) -> Tuple[int, float]:\n",
    "        epsilon = 1 - (episode + 1) / self.training_length\n",
    "        if epsilon > np.random.rand():\n",
    "            a = np.random.randint(env.action_space.n)\n",
    "        else:\n",
    "            a = np.argmax(self.q_target[s, :])\n",
    "        return a, epsilon\n",
    "\n",
    "    def train(self, env):\n",
    "        # Initialize the model / q table with zeros/random\n",
    "        self.q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "        # Create a target model / q table\n",
    "        self.q_target = self.q\n",
    "\n",
    "        ### METRICS\n",
    "        # create lists to contain various metrics that should be tracked during the training process\n",
    "        self.metrics = {\n",
    "            \"return\": np.zeros(self.training_length),\n",
    "            \"q_avg\": np.zeros(self.training_length),\n",
    "            \"epsilon\": np.zeros(self.training_length),\n",
    "            \"td_error\": np.zeros(self.training_length),\n",
    "        }\n",
    "\n",
    "        for episode in range(self.training_length):\n",
    "            # Reset the environment and retrieve the initial state\n",
    "            s, _ = env.reset()\n",
    "            # Set the 'done' flag to false\n",
    "            d = False\n",
    "            # Set the step of the episode to 0\n",
    "            step = 0\n",
    "            # Start the Q-Learning algorithm\n",
    "            while step < self.episode_length:\n",
    "                # Derive action from current policy (epsilon_greedy noise)\n",
    "                a, epsilon = self.epsilon_greedy_noise(env, s, episode)\n",
    "\n",
    "                # Execute the action and generate a successor state as well as receive an immediate reward\n",
    "                s1, r, d, _, _ = env.step(a)\n",
    "\n",
    "                # Calculate the temporal difference error\n",
    "                td_e = self.td_error(self.q_target, s, a, s1, r)\n",
    "\n",
    "                # Create a transition tuple\n",
    "                transition = Transition(-(td_e + 0.001), s, a, s1, r, td_e)\n",
    "\n",
    "                # Update model / q table\n",
    "                self.q[s, a] = self.td_update(self.q_target[s, a], td_e)\n",
    "\n",
    "                # Assign the current state the value of the successor state\n",
    "                s = s1\n",
    "\n",
    "                # Increment the step\n",
    "                step += 1\n",
    "\n",
    "                ### METRICS\n",
    "                # Accumulate the episode return\n",
    "                self.metrics[\"return\"][episode] += self.discount_factor**step * r\n",
    "                # Track the temporal difference error\n",
    "                self.metrics[\"td_error\"][episode] += td_e\n",
    "                # Track the max epsilon values\n",
    "                self.metrics[\"epsilon\"][episode] += np.max(epsilon)\n",
    "                # Track the average q values\n",
    "                self.metrics[\"q_avg\"][episode] = np.average(self.q)\n",
    "\n",
    "                # If we reached a terminal state abort the while loop reset the environment and start over\n",
    "                if d == True or step == 100:\n",
    "                    # At the end of the episode update the target model with the current model\n",
    "\n",
    "                    self.q_target = self.q\n",
    "\n",
    "                    ### METRICS\n",
    "                    self.metrics[\"epsilon\"][episode] /= step\n",
    "                    self.metrics[\"q_avg\"][episode] /= step\n",
    "                    self.metrics[\"td_error\"][episode] /= step\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Tuple\n",
    "For ease of use we define a configuration dataclass that allows us to combine all the relevant configuration from into one object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ConfigQAgent:\n",
    "    learning_rate: float\n",
    "    training_length: int\n",
    "    episode_length: int\n",
    "    discount_factor: float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and Train the Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_q_agent = ConfigQAgent(\n",
    "    learning_rate=0.1,\n",
    "    training_length=400,\n",
    "    episode_length=50,\n",
    "    discount_factor=DISCOUNT_FACTOR,\n",
    ")\n",
    "\n",
    "q_agent = QAgent(config_q_agent)\n",
    "q_agent.train(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions to Compute Value Function and Policy\n",
    "\n",
    "- `compute_v_from_q`:\n",
    "Given a state-action value function $q$ (represented as a matrix), the state value function $v$ is computed for each state $s$:\n",
    "\n",
    "$ v(s) = \\max_{a} q(s, a) $\n",
    "\n",
    "where $ a $ denotes the action.\n",
    "\n",
    "- `compute_policy_from_q`:\n",
    "Given the same state-action value function $ q $, the policy function $ \\pi $ for each state $ s $ is computed as:\n",
    "\n",
    "$ \\pi(s) = \\arg\\max_{a} q(s, a) $\n",
    "\n",
    "where $ \\arg\\max $ returns the action $ a $ that maximizes the value of $ q(s, a) $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_v_from_q(env, q: np.ndarray) -> float:\n",
    "    \"\"\"Compute the v function given the q function, maximizing over the actions of a given state.\"\"\"\n",
    "    # Initialize the state-value function to zero\n",
    "    v = np.zeros(env.observation_space.n)\n",
    "    for s in range(env.observation_space.n):\n",
    "        # The (optimal) state-value function is the maximum of the q function over the actions\n",
    "        v[s] = np.max(q[s, :])\n",
    "    return v\n",
    "\n",
    "\n",
    "def compute_policy_from_q(env, q: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute the policy function given the q function, finding the action that yields the maximum of a given state.\"\"\"\n",
    "    # Initialize the policy (function) to zero (left)\n",
    "    policy = np.zeros(env.observation_space.n, dtype=int)\n",
    "    for s in range(env.observation_space.n):\n",
    "        # The (optimal) policy (function) is the action that yields the maximum of the q function over the actions\n",
    "        policy[s] = np.argmax(q[s, :])\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = compute_policy_from_q(env, q_agent.q_target)\n",
    "v = compute_v_from_q(env, q_agent.q_target)\n",
    "\n",
    "print(\n",
    "    f\"Average return: {calculate_policy_return(env, policy, q_agent.discount_factor):.2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Score over time: {sum(q_agent.metrics['return']) / q_agent.training_length:.2f}\"\n",
    ")\n",
    "\n",
    "fig, axi = plt.subplots(1, 2)\n",
    "visualize_p(env, v, policy, Action, axi[0], title=\"Policy for the Frozen Lake\")\n",
    "visualize_v(env, v, axi[1], title=\"State Value Function for the Frozen Lake\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 4)\n",
    "# Plot the return over time\n",
    "ax[0].plot(range(q_agent.training_length), q_agent.metrics[\"return\"], \".\")\n",
    "ax[0].set(xlabel=\"episode\", ylabel=\"reward\", title=\"Return\")\n",
    "ax[0].grid()\n",
    "\n",
    "# Plot the Q value over time\n",
    "ax[1].plot(range(q_agent.training_length), q_agent.metrics[\"q_avg\"], \".\")\n",
    "ax[1].set(xlabel=\"episode\", ylabel=\"Q Value\", title=\"Average Q Value\")\n",
    "ax[1].grid()\n",
    "\n",
    "# Plot the epsilon over time\n",
    "ax[2].plot(range(q_agent.training_length), q_agent.metrics[\"epsilon\"], \".\")\n",
    "ax[2].set(xlabel=\"episode\", ylabel=\"epsilon\", title=\"Epsilon\")\n",
    "ax[2].grid()\n",
    "\n",
    "# Plot the td error over time\n",
    "ax[3].plot(range(q_agent.training_length), q_agent.metrics[\"td_error\"], \".\")\n",
    "ax[3].set(xlabel=\"episode\", ylabel=\"TD Error\", title=\"TD Error\")\n",
    "ax[3].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate different Hyperparameters (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_training_episodes(number_evaluation_points: int, number_evaluations: int):\n",
    "    \"\"\"Evaluate the training process for different number of training episodes\"\"\"\n",
    "    training_lengths = np.linspace(1, 501, number_evaluation_points, dtype=int)\n",
    "    returns = np.zeros(number_evaluation_points)\n",
    "    for i in range(number_evaluation_points):\n",
    "        config_q_agent = ConfigQAgent(\n",
    "            learning_rate=0.1,\n",
    "            training_length=training_lengths[i],\n",
    "            episode_length=100,\n",
    "            discount_factor=DISCOUNT_FACTOR,\n",
    "        )\n",
    "        for j in range(number_evaluations):\n",
    "            q_agent = QAgent(config_q_agent)\n",
    "            q_agent.train(env)\n",
    "            returns[i] += np.max(q_agent.metrics[\"return\"])\n",
    "        returns[i] /= number_evaluations\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set(xlabel=\"#Episodes\", ylabel=\"Max. Return\", title=\"#Episodes vs. Return\")\n",
    "    ax.plot(training_lengths, returns, \"-o\")\n",
    "\n",
    "\n",
    "evaluate_training_episodes(10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would you train an rl agent in reality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "# Train the agent\n",
    "model = DQN(\"MlpPolicy\", env).learn(100000)\n",
    "\n",
    "# Extract the policy from the model\n",
    "p = np.zeros(env.observation_space.n)\n",
    "for state in range(env.observation_space.n):\n",
    "    p[state], _ = model.predict(state)\n",
    "\n",
    "# Visualize the policy\n",
    "_, ax = plt.subplots()\n",
    "visualize_p(env, None, p, Action, ax, title=\"Policy for the Frozen Lake (DQN)\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6wYUHIokU_EI",
    "8MH3Ij6rAL_z",
    "JWdytOiH-LFr",
    "GzgwlDeZhfxU",
    "rTC-P1vd-5-y",
    "-pzYcAtuiHJ9",
    "zhrrLKXk0ElG",
    "CASyoXI9jAZW",
    "lU4gmOQcAjR_",
    "4CdfVP4DilJf",
    "wK6bzLs_iqeG",
    "5KUNPRHdAstO",
    "tny1fTdaIkR6"
   ],
   "name": "Exercise 04 - Reinforcement Learning with Gym and Pytorch.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "rl-kernel",
   "language": "python",
   "name": "rl-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
